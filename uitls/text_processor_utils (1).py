import re
import numpy as np
import streamlit as st
from tensorflow.keras.preprocessing.sequence import pad_sequences
import logging

logger = logging.getLogger(__name__)

class TextProcessor:
    """ÙƒÙ„Ø§Ø³ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ"""
    
    def __init__(self, max_len=100):
        self.max_len = max_len
    
    @staticmethod
    def clean_text(text):
        """ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†Øµ"""
        if not text or not isinstance(text, str):
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'\s+', ' ', text.strip())
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
        text = re.sub(r'[!@#$%^&*()_+=\[\]{}|;:,.<>?~`]{3,}', ' ', text)
        
        # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ù‡Ù…Ø©
        text = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF.,!?;:]', ' ', text)
        
        return text.strip()
    
    @staticmethod
    def validate_text(text, min_length=10, max_length=10000):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Øµ"""
        if not text:
            return False, "ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"
        
        if len(text) < min_length:
            return False, f"Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ {min_length} Ø­Ø±Ù)"
        
        if len(text) > max_length:
            return False, f"Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ {max_length} Ø­Ø±Ù)"
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª ÙØ¹Ù„ÙŠØ©
        words = text.split()
        if len(words) < 3:
            return False, "Ø§Ù„Ù†Øµ ÙŠØ­ØªØ§Ø¬ Ù„Ø£ÙƒØ«Ø± Ù…Ù† 3 ÙƒÙ„Ù…Ø§Øª"
        
        return True, "Ø§Ù„Ù†Øµ ØµØ§Ù„Ø­ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"
    
    def preprocess_for_classification(self, text, tokenizer):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù„Ù„ØªØµÙ†ÙŠÙ"""
        try:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
            cleaned_text = self.clean_text(text)
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ sequences
            sequences = tokenizer.texts_to_sequences([cleaned_text])
            
            # padding
            padded = pad_sequences(
                sequences, 
                maxlen=self.max_len, 
                padding='post', 
                truncating='post'
            )
            
            return padded
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù„Ù„ØªØµÙ†ÙŠÙ: {e}")
            raise e
    
    @staticmethod
    def preprocess_for_summarization(text, max_chunk_size=1024):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù„Ù„ØªÙ„Ø®ÙŠØµ"""
        try:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
            cleaned_text = TextProcessor.clean_text(text)
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
            if len(cleaned_text) > max_chunk_size:
                # ØªÙ‚Ø³ÙŠÙ… Ø¹Ù†Ø¯ Ø§Ù„Ø¬Ù…Ù„
                sentences = re.split(r'[.!?]', cleaned_text)
                
                chunks = []
                current_chunk = ""
                
                for sentence in sentences:
                    if len(current_chunk + sentence) < max_chunk_size:
                        current_chunk += sentence + ". "
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence + ". "
                
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                return chunks
            
            return [cleaned_text]
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù„Ù„ØªÙ„Ø®ÙŠØµ: {e}")
            return [text]  # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
    
    @staticmethod
    def get_text_statistics(text):
        """Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Øµ"""
        if not text:
            return {
                'characters': 0,
                'words': 0,
                'sentences': 0,
                'paragraphs': 0,
                'avg_word_length': 0,
                'reading_time': 0
            }
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù
        characters = len(text)
        
        # Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        words = len(text.split())
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„
        sentences = len(re.findall(r'[.!?]+', text))
        
        # Ø¹Ø¯Ø¯ Ø§Ù„ÙÙ‚Ø±Ø§Øª
        paragraphs = len([p for p in text.split('\n\n') if p.strip()])
        
        # Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©
        word_lengths = [len(word) for word in text.split()]
        avg_word_length = np.mean(word_lengths) if word_lengths else 0
        
        # ÙˆÙ‚Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠ (200 ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©)
        reading_time = round(words / 200, 1) if words > 0 else 0
        
        return {
            'characters': characters,
            'words': words,
            'sentences': max(sentences, 1),  # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©
            'paragraphs': max(paragraphs, 1),  # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ÙÙ‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
            'avg_word_length': round(avg_word_length, 1),
            'reading_time': reading_time
        }
    
    @staticmethod
    def classify_text_complexity(word_count):
        """ØªØµÙ†ÙŠÙ Ù…Ø³ØªÙˆÙ‰ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Øµ"""
        if word_count < 50:
            return "Ù‚ØµÙŠØ±", "ğŸŸ¢"
        elif word_count < 200:
            return "Ù…ØªÙˆØ³Ø·", "ğŸŸ¡"
        elif word_count < 500:
            return "Ø·ÙˆÙŠÙ„", "ğŸŸ "
        else:
            return "Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹", "ğŸ”´"
    
    @staticmethod
    def suggest_processing_options(text_stats):
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ"""
        suggestions = []
        
        word_count = text_stats['words']
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ØªØµÙ†ÙŠÙ
        if word_count >= 10:
            suggestions.append("âœ… Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªØµÙ†ÙŠÙ")
        else:
            suggestions.append("âš ï¸ Ù‚ØµÙŠØ± Ù„Ù„ØªØµÙ†ÙŠÙ (ÙŠØ­ØªØ§Ø¬ 10+ ÙƒÙ„Ù…Ø§Øª)")
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ØªÙ„Ø®ÙŠØµ
        if word_count >= 100:
            suggestions.append("âœ… Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªÙ„Ø®ÙŠØµ")
        elif word_count >= 50:
            suggestions.append("ğŸŸ¡ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ„Ø®ÙŠØµ (Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…Ø®ØªØµØ±Ø§Ù‹)")
        else:
            suggestions.append("âŒ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØªÙ„Ø®ÙŠØµ (ÙŠØ­ØªØ§Ø¬ 50+ ÙƒÙ„Ù…Ø©)")
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„Ø³Ø±Ø¹Ø©
        if word_count > 1000:
            suggestions.append("â° Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø·ÙˆÙ„")
        
        return suggestions