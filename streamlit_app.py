import streamlit as st
import tensorflow as tf
import pickle
import os
import numpy as np
from transformers import pipeline
import logging
import traceback

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Streamlit
st.set_page_config(
    page_title="Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

class ModelLoader:
    """ÙƒÙ„Ø§Ø³ Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    
    def __init__(self, models_path='models/'):
        self.models_path = models_path
        self.model = None
        self.tokenizer = None
        self.label_encoder = None
        self.summarizer = None
        
    @st.cache_resource
    def load_classification_model(_self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ùˆ Tokenizer Ùˆ Label Encoder"""
        try:
            model_path = os.path.join(_self.models_path, 'lstm_simple.h5')
            tokenizer_path = os.path.join(_self.models_path, 'tokenizer.pkl')
            encoder_path = os.path.join(_self.models_path, 'label_encoder.pkl')
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª
            missing_files = []
            for path, name in [(model_path, 'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬'), (tokenizer_path, 'Tokenizer'), (encoder_path, 'Label Encoder')]:
                if not os.path.exists(path):
                    missing_files.append(f"Ù…Ù„Ù {name}: {path}")
            
            if missing_files:
                logger.warning("Ù…Ù„ÙØ§Øª Ù…ÙÙ‚ÙˆØ¯Ø©:")
                for file in missing_files:
                    logger.warning(f"  - {file}")
                return None, None, None
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ...")
            model = tf.keras.models.load_model(model_path)
            
            # ØªØ­Ù…ÙŠÙ„ Tokenizer
            logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Tokenizer...")
            with open(tokenizer_path, 'rb') as f:
                tokenizer = pickle.load(f)
            
            # ØªØ­Ù…ÙŠÙ„ Label Encoder
            logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Label Encoder...")
            with open(encoder_path, 'rb') as f:
                label_encoder = pickle.load(f)
            
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ù†Ø¬Ø§Ø­")
            return model, tokenizer, label_encoder
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ: {e}")
            return None, None, None
    
    @st.cache_resource
    def load_summarization_model(_self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ"""
        try:
            logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ...")
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ± ÙˆØ£Ø³Ø±Ø¹ Ù„Ù„Ø¥Ù†ØªØ§Ø¬
            summarizer = pipeline(
                "summarization", 
                model="sshleifer/distilbart-cnn-12-6",  # Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ± ÙˆØ£Ø³Ø±Ø¹
                device=-1,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU
                framework="pt"
            )
            
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø¨Ù†Ø¬Ø§Ø­")
            return summarizer
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {e}")
            return None
    
    def load_all_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ
        self.model, self.tokenizer, self.label_encoder = self.load_classification_model()
        
        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ
        self.summarizer = self.load_summarization_model()
        
        return (self.model is not None and 
                self.tokenizer is not None and 
                self.label_encoder is not None)
    
    def get_models(self):
        """Ø¥Ø±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        return self.model, self.tokenizer, self.label_encoder, self.summarizer
    
    def is_classification_ready(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ"""
        return all([self.model, self.tokenizer, self.label_encoder])
    
    def is_summarization_ready(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ"""
        return self.summarizer is not None

def preprocess_text(text, tokenizer, max_length=100):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù‚Ø¨Ù„ Ø§Ù„ØªØµÙ†ÙŠÙ"""
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„
        sequence = tokenizer.texts_to_sequences([text])
        
        # Ø¥Ø¶Ø§ÙØ© padding
        padded = tf.keras.preprocessing.sequence.pad_sequences(
            sequence, maxlen=max_length, padding='post'
        )
        
        return padded
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ: {e}")
        return None

def classify_text(text, model, tokenizer, label_encoder):
    """ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ"""
    try:
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
        processed_text = preprocess_text(text, tokenizer)
        if processed_text is None:
            return None, None
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        prediction = model.predict(processed_text, verbose=0)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
        predicted_class_idx = np.argmax(prediction[0])
        predicted_class = label_encoder.inverse_transform([predicted_class_idx])[0]
        confidence = float(prediction[0][predicted_class_idx])
        
        return predicted_class, confidence
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ: {e}")
        return None, None

def summarize_text(text, summarizer, max_length=150, min_length=50):
    """ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ"""
    try:
        if len(text.split()) < 10:
            return "Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØªÙ„Ø®ÙŠØµ"
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
        max_input_length = 1024
        if len(text) > max_input_length:
            text = text[:max_input_length]
        
        # ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ
        summary = summarizer(
            text,
            max_length=max_length,
            min_length=min_length,
            do_sample=False
        )
        
        return summary[0]['summary_text']
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ: {e}")
        return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {str(e)}"

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚"""
    
    # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    st.title("ğŸ¤– Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ")
    st.markdown("---")
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        st.subheader("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬..."):
            model_loader = ModelLoader()
            classification_loaded = model_loader.load_all_models()
            model, tokenizer, label_encoder, summarizer = model_loader.get_models()
        
        # Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        if model_loader.is_classification_ready():
            st.success("âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ: Ù…ØªØ§Ø­")
        else:
            st.error("âŒ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ: ØºÙŠØ± Ù…ØªØ§Ø­")
        
        if model_loader.is_summarization_ready():
            st.success("âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ: Ù…ØªØ§Ø­")
        else:
            st.warning("âš ï¸ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ: ØºÙŠØ± Ù…ØªØ§Ø­")
        
        st.markdown("---")
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        st.subheader("ğŸ›ï¸ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
        show_confidence = st.checkbox("Ø¹Ø±Ø¶ Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©", value=True)
        auto_summarize = st.checkbox("ØªÙ„Ø®ÙŠØµ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©", value=False)
    
    # Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ“ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ")
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        input_method = st.radio(
            "Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ:",
            ["ÙƒØªØ§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©", "Ø±ÙØ¹ Ù…Ù„Ù Ù†ØµÙŠ"],
            horizontal=True
        )
        
        user_text = ""
        
        if input_method == "ÙƒØªØ§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©":
            user_text = st.text_area(
                "Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù„Ù„ØªØµÙ†ÙŠÙ ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ:",
                height=200,
                placeholder="Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§..."
            )
        else:
            uploaded_file = st.file_uploader(
                "Ø§Ø®ØªØ± Ù…Ù„Ù Ù†ØµÙŠ",
                type=['txt'],
                help="ÙŠØ¯Ø¹Ù… Ù…Ù„ÙØ§Øª .txt ÙÙ‚Ø·"
            )
            
            if uploaded_file is not None:
                try:
                    user_text = str(uploaded_file.read(), "utf-8")
                    st.success(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­! Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù: {len(user_text)}")
                except Exception as e:
                    st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
    
    with col2:
        st.header("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Øµ")
        
        if user_text:
            words_count = len(user_text.split())
            chars_count = len(user_text)
            
            st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", words_count)
            st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù", chars_count)
            
            if words_count > 200:
                st.info("Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„ - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªÙ„Ø®ÙŠØµ")
            elif words_count < 10:
                st.warning("Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± - Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„ØªÙ„Ø®ÙŠØµ Ù…ÙÙŠØ¯Ø§Ù‹")
    
    # Ø²Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    if st.button("ğŸš€ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ", type="primary", use_container_width=True):
        if not user_text.strip():
            st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„")
        else:
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø©
            results_col1, results_col2 = st.columns(2)
            
            with results_col1:
                st.subheader("ğŸ·ï¸ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØµÙ†ÙŠÙ")
                
                if model_loader.is_classification_ready():
                    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ..."):
                        predicted_class, confidence = classify_text(
                            user_text, model, tokenizer, label_encoder
                        )
                    
                    if predicted_class is not None:
                        st.success(f"**Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:** {predicted_class}")
                        if show_confidence:
                            st.info(f"**Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©:** {confidence:.2%}")
                            
                            # Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©
                            st.progress(confidence)
                    else:
                        st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ")
                else:
                    st.error("âŒ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ ØºÙŠØ± Ù…ØªØ§Ø­")
            
            with results_col2:
                st.subheader("ğŸ“„ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Øµ")
                
                if model_loader.is_summarization_ready():
                    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ..."):
                        summary = summarize_text(user_text, summarizer)
                    
                    if summary:
                        st.success("**Ø§Ù„Ù…Ù„Ø®Øµ:**")
                        st.write(summary)
                    else:
                        st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ")
                else:
                    st.warning("âš ï¸ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ ØºÙŠØ± Ù…ØªØ§Ø­")
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    st.markdown("---")
    with st.expander("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…"):
        st.markdown("""
        ### Ø­ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…
        
        Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ‚ÙˆÙ… Ø¨Ù€:
        
        - **ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ**: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ LSTM Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø®ØªÙ„ÙØ©
        - **ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ**: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ BART Ø§Ù„Ù…ÙØ¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
        
        ### Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©
        
        - Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ: `lstm_simple.h5`
        - Ù…Ù„Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: `tokenizer.pkl`  
        - Ù…Ù„Ù ØªØ±Ù…ÙŠØ² Ø§Ù„ÙØ¦Ø§Øª: `label_encoder.pkl`
        - Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ: `distilbart-cnn-12-6`
        
        ### Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        
        - Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù†ØµÙˆØµ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©
        - Ù„Ù„ØªÙ„Ø®ÙŠØµØŒ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© (Ø£ÙƒØ«Ø± Ù…Ù† 50 ÙƒÙ„Ù…Ø©) ØªØ¹Ø·ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„
        - ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© Ø¨ØµÙŠØºØ© .txt Ù…Ø¨Ø§Ø´Ø±Ø©
        """)

    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø­Ø§Ù„ Ø¹Ø¯Ù… ØªÙˆÙØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    if not model_loader.is_classification_ready() and not model_loader.is_summarization_ready():
        st.error("""
        âŒ **Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­**
        
        ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ù…Ø¬Ù„Ø¯ `models/`:
        - `lstm_simple.h5` (Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ)
        - `tokenizer.pkl` (Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ)
        - `label_encoder.pkl` (ØªØ±Ù…ÙŠØ² Ø§Ù„ÙØ¦Ø§Øª)
        
        Ø£Ùˆ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ø®ÙŠØµ.
        """)

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚: {e}")
        st.error("ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø·Ø£:")
        st.code(traceback.format_exc())
